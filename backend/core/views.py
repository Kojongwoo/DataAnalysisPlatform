# backend/core/views.py

import pandas as pd
import io
import numpy as np
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.parsers import MultiPartParser, JSONParser

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVC, SVR

# --- í—¬í¼ í•¨ìˆ˜ ---
def _analyze_dataframe(df):
    """
    ì£¼ì–´ì§„ DataFrameì„ ë¶„ì„í•˜ì—¬ table, stats, quality JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    **ì„±ëŠ¥ ìµœì í™”**: í”„ë¡ íŠ¸ì—”ë“œ ë Œë”ë§ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ tableDataëŠ” ìƒìœ„ 100ê°œ í–‰ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
# --- 1. ì „ì²´ í…Œì´ë¸” ë°ì´í„° (Previewìš© 100ê°œë§Œ) ---
    # ğŸ’¡ ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ë³´ë‚´ë©´ ë¸Œë¼ìš°ì €ê°€ ë©ˆì¶¥ë‹ˆë‹¤. ìƒìœ„ 100ê°œë§Œ ìë¦…ë‹ˆë‹¤.
    df_preview = df.head(100).copy()
    
    # ğŸ’¡ [Warning í•´ê²°] fillna ëŒ€ì‹  whereë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´('-')ë¡œ ë³€í™˜
    df_preview = df_preview.astype(object).where(pd.notnull(df_preview), '-')
    
    table_json = df_preview.to_json(orient='split', force_ascii=False)

    # --- 2. ê¸°ì´ˆ í†µê³„ëŸ‰ ë°ì´í„° ---
    stats_df = df.describe(include='all')
    
    # (2) ë°ì´í„° íƒ€ì…(Data Type) í–‰ ìƒì„±
    # ê° ì»¬ëŸ¼ì´ ìˆ˜ì¹˜í˜•ì¸ì§€ ì•„ë‹Œì§€ íŒë³„
    dtype_data = {}
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            dtype_data[col] = 'Numeric (ìˆ˜ì¹˜í˜•)'
        else:
            dtype_data[col] = 'Categorical (ë²”ì£¼í˜•)'
            
    # DataFrameìœ¼ë¡œ ë³€í™˜ (ì¸ë±ìŠ¤ ì´ë¦„ì€ 'Data Type')
    dtype_df = pd.DataFrame([dtype_data], index=['Data Type'])
    stats_df = pd.concat([dtype_df, stats_df])
    
    stats_df = stats_df.reset_index()
    stats_df.rename(columns={'index': 'êµ¬ë¶„'}, inplace=True)
    
    # ğŸ’¡ [Warning í•´ê²°] stats_df ì²˜ë¦¬
    stats_df = stats_df.astype(object).where(pd.notnull(stats_df), '-')
    stats_json = stats_df.to_json(orient='split', force_ascii=False)
    # --------------------------------------

    # 3. ë°ì´í„° í’ˆì§ˆ ë°ì´í„°
    total_rows = len(df)
    missing_counts = df.isnull().sum()

    if total_rows > 0:
        missing_percent = (missing_counts / total_rows * 100).round(2)
    else:
        missing_percent = pd.Series(0.0, index=df.columns)
    
    outlier_counts = pd.Series('-', index=df.columns)
    outlier_percent = pd.Series('-', index=df.columns)
    
    # ìˆ˜ì¹˜í˜• ë³€í™˜ ì‹œë„ (ì´ìƒì¹˜ ê³„ì‚°ì„ ìœ„í•´)
    # object íƒ€ì…ì´ë¼ë„ ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•˜ë‹¤ë©´ ë³€í™˜í•´ì„œ ê³„ì‚°
    # ğŸ’¡ ìˆ˜ì •: errors='ignore' ëŒ€ì‹  coerceë¡œ ê°•ì œ ë³€í™˜ í›„ ìˆ˜ì¹˜í˜•ë§Œ ì„ íƒ
    df_numeric = df.copy()
    for col in df_numeric.columns:
        try:
            df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')
        except:
            pass
            
    numeric_cols = df_numeric.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        Q1 = df_numeric[col].quantile(0.25)
        Q3 = df_numeric[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - (1.5 * IQR)
        upper_bound = Q3 + (1.5 * IQR)
        
        count = ((df_numeric[col] < lower_bound) | (df_numeric[col] > upper_bound)).sum()
        outlier_counts[col] = count

        if total_rows > 0:
            outlier_percent[col] = (count / total_rows * 100).round(2)
        else:
            outlier_percent[col] = 0.0
        
    quality_df = pd.DataFrame({
        'ê²°ì¸¡ì¹˜ ê°œìˆ˜': missing_counts,
        'ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)': missing_percent,
        'ì´ìƒì¹˜ ê°œìˆ˜': outlier_counts,
        'ì´ìƒì¹˜ ë¹„ìœ¨(%)': outlier_percent
    })
    
    quality_df = quality_df.transpose().reset_index()
    quality_df.rename(columns={'index': 'êµ¬ë¶„'}, inplace=True)

    quality_df.replace(np.nan, '-', inplace=True)
    
    # ğŸ’¡ ìˆ˜ì •: ê²½ê³  ë°©ì§€
    quality_json = quality_df.astype(object).fillna('-').to_json(orient='split', force_ascii=False)

    return {
        'tableData': table_json,
        'statsData': stats_json,
        'qualityData': quality_json
    }

class FileUploadView(APIView):
    parser_classes = (MultiPartParser,)

    def post(self, request, *args, **kwargs):
        file_obj = request.FILES.get('file')

        if not file_obj:
            return Response({"error": "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}, status=400)

        try:
            file_buffer = io.BytesIO(file_obj.read())

            if file_obj.name.endswith(('.xls', '.xlsx')):
                df = pd.read_excel(file_buffer)
            elif file_obj.name.endswith('.csv'):
                try:
                    df = pd.read_csv(file_buffer)
                except UnicodeDecodeError:
                    file_buffer.seek(0)
                    df = pd.read_csv(file_buffer, encoding='cp949')
            else:
                return Response({"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."}, status=400)
            
            # ğŸ’¡ [ìˆ˜ì • 1] ë°ì´í„°ì…‹ íŠ¹í™” ì „ì²˜ë¦¬: '?'ë¥¼ NaN(ê²°ì¸¡ì¹˜)ìœ¼ë¡œ ë³€í™˜
            df.replace('?', np.nan, inplace=True)

            response_data = _analyze_dataframe(df)
            response_data['fullData'] = df.to_json(orient='split', force_ascii=False)
            
            return Response(response_data)

        except Exception as e:
            return Response({"error": f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}, status=500)

class ProcessDataView(APIView):
    parser_classes = (JSONParser,)
    
    def post(self, request, *args, **kwargs):
        df_json = request.data.get('dataframe')
        action = request.data.get('action')

        if not df_json:
            return Response({"error": "DataFrameì´ ìš”ì²­ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}, status=400)
        
        try:
            # DataFrame ë³µì›
            df = pd.read_json(io.StringIO(df_json), orient='split')
            original_rows = len(df)
            
            # 1. ë¹ˆ ë¬¸ìì—´ -> NaN ì¹˜í™˜
            df.replace("", np.nan, inplace=True)
            # ğŸ’¡ [ìˆ˜ì • 2] '?' -> NaN ì¹˜í™˜ ì¶”ê°€
            df.replace("?", np.nan, inplace=True)

            # 2. ìˆ˜ì¹˜í˜• ë³€í™˜ ì‹œë„
            for col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col])
                except (ValueError, TypeError):
                    pass

            if action == 'drop_na':
                df = df.dropna()
                print(f"ê²°ì¸¡ì¹˜ í–‰ ì œê±°: {original_rows} -> {len(df)}")
            
            elif action == 'fill_na_mean':
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
                    print(f"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í‰ê· ê°’ ëŒ€ì²´ ì™„ë£Œ: {list(numeric_cols)}")
                # ğŸ’¡ ì£¼ì˜: ë¬¸ìì—´ ì»¬ëŸ¼ì€ ì—¬ê¸°ì„œ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ

            elif action == 'fill_na_median':
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
                    print(f"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ì•™ê°’ ëŒ€ì²´ ì™„ë£Œ: {list(numeric_cols)}")

            # --- ğŸ’¡ [ì‹ ê·œ ì¶”ê°€] ìµœë¹ˆê°’(Mode)ìœ¼ë¡œ ì±„ìš°ê¸° ---
            elif action == 'fill_na_mode':
                # ëª¨ë“  ì»¬ëŸ¼ì„ ìˆœíšŒí•˜ë©° ê²°ì¸¡ì¹˜ê°€ ìˆìœ¼ë©´ ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›€
                filled_cols = []
                for col in df.columns:
                    if df[col].isnull().sum() > 0:
                        # ìµœë¹ˆê°’ì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸([0])ë¥¼ ì„ íƒ
                        mode_value = df[col].mode()[0]
                        df[col] = df[col].fillna(mode_value)
                        filled_cols.append(col)
                print(f"ìµœë¹ˆê°’ ëŒ€ì²´ ì™„ë£Œ (ëŒ€ìƒ ì»¬ëŸ¼): {filled_cols}")

            elif action == 'fill_na_zero':
                df.fillna(0, inplace=True)
                print("ëª¨ë“  ì»¬ëŸ¼ 0ìœ¼ë¡œ ëŒ€ì²´ ì™„ë£Œ")

            # --- ğŸ’¡ [ì‹ ê·œ ì¶”ê°€] ì´ìƒì¹˜ ì²˜ë¦¬ ë¡œì§ ---
            elif action == 'drop_outliers':
                # IQR ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì‹ë³„ í›„ ì œê±°
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                Q1 = df[numeric_cols].quantile(0.25)
                Q3 = df[numeric_cols].quantile(0.75)
                IQR = Q3 - Q1
                
                # ì¡°ê±´: (ê°’ < Lower) ë˜ëŠ” (ê°’ > Upper) ì¸ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” í–‰ ì œê±°
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # any(axis=1)ì€ "ê° í–‰ì— ëŒ€í•´ í•˜ë‚˜ë¼ë„ Trueê°€ ìˆìœ¼ë©´ True"
                outlier_condition = ((df[numeric_cols] < lower_bound) | (df[numeric_cols] > upper_bound)).any(axis=1)
                
                original_rows = len(df)
                df = df[~outlier_condition] # Outlierê°€ ì•„ë‹Œ ê²ƒë§Œ ë‚¨ê¹€
                print(f"ì´ìƒì¹˜ í¬í•¨ í–‰ ì œê±°: {original_rows} -> {len(df)}")

            elif action == 'cap_outliers':
                # ìœˆì €ë¼ì´ì§• (Capping): ì´ìƒì¹˜ë¥¼ ìƒí•œê°’/í•˜í•œê°’ìœ¼ë¡œ ëŒ€ì²´
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                Q1 = df[numeric_cols].quantile(0.25)
                Q3 = df[numeric_cols].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                for col in numeric_cols:
                    # í•˜í•œê°’ë³´ë‹¤ ì‘ì€ ê°’ì€ í•˜í•œê°’ìœ¼ë¡œ ì¹˜í™˜
                    df[col] = np.where(df[col] < lower_bound[col], lower_bound[col], df[col])
                    # ìƒí•œê°’ë³´ë‹¤ í° ê°’ì€ ìƒí•œê°’ìœ¼ë¡œ ì¹˜í™˜
                    df[col] = np.where(df[col] > upper_bound[col], upper_bound[col], df[col])
                print("ì´ìƒì¹˜ ìœˆì €ë¼ì´ì§•(Capping) ì™„ë£Œ")
            
            else:
                return Response({"error": "ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… ìš”ì²­ì…ë‹ˆë‹¤."}, status=400)

            response_data = _analyze_dataframe(df)
            response_data['fullData'] = df.to_json(orient='split', force_ascii=False)
            
            return Response(response_data)

        except Exception as e:
            import traceback
            traceback.print_exc()
            return Response({"error": f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}, status=500)

class TrainModelView(APIView):
    parser_classes = (JSONParser,)

    def post(self, request, *args, **kwargs):
        df_json = request.data.get('dataframe')
        target_col = request.data.get('target')
        model_name = request.data.get('model_name', 'rf') # ğŸ’¡ ê¸°ë³¸ê°’ 'rf' (Random Forest)

        if not df_json or not target_col:
            return Response({"error": "ë°ì´í„° ë˜ëŠ” ëª©í‘œ ì»¬ëŸ¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}, status=400)

        try:
            # 1. ë°ì´í„° ë³µì›
            df = pd.read_json(io.StringIO(df_json), orient='split')
            
            # 2. ë°ì´í„° ì „ì²˜ë¦¬ (ID ì»¬ëŸ¼ ì œê±° ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬)
            cols_to_drop = [c for c in df.columns if 'ID' in c or 'id' in c or 'nbr' in c]
            df_clean = df.drop(columns=cols_to_drop, errors='ignore')

            if target_col in df_clean.columns:
                df_clean = df_clean.dropna(subset=[target_col])
            
            for col in df_clean.columns:
                if df_clean[col].isnull().sum() > 0:
                    # ìˆ˜ì¹˜í˜•ì´ë©´ í‰ê· , ì•„ë‹ˆë©´ ìµœë¹ˆê°’ (ê°„ë‹¨í•œ ì²˜ë¦¬)
                    if pd.api.types.is_numeric_dtype(df_clean[col]):
                        df_clean[col] = df_clean[col].fillna(df_clean[col].mean())
                    else:
                        df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])

            if target_col not in df_clean.columns:
                 return Response({"error": f"ëª©í‘œ ì»¬ëŸ¼ '{target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, status=400)

            # 3. ëª©í‘œ ë³€ìˆ˜(y) ë¶„ë¦¬ ë° íƒ€ì… íŒë‹¨
            y = df_clean[target_col]
            X = df_clean.drop(columns=[target_col])

            is_regression = False
            if pd.api.types.is_numeric_dtype(y):
                if pd.api.types.is_float_dtype(y) or y.nunique() > 20:
                    is_regression = True

            # 4. ì¸ì½”ë”©
            for col in X.select_dtypes(include=['object']).columns:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

            if not is_regression and y.dtype == 'object':
                le_y = LabelEncoder()
                y = le_y.fit_transform(y.astype(str))

            # 5. ë°ì´í„° ë¶„ë¦¬
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # 6. ğŸ’¡ ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ (ë¶„ê¸° ì²˜ë¦¬)
            model = None
            
            if is_regression:
                # --- íšŒê·€ (Regression) ---
                if model_name == 'linear':
                    model = LinearRegression()
                elif model_name == 'gb':
                    model = GradientBoostingRegressor(n_estimators=100, random_state=42)
                elif model_name == 'svm':
                    model = SVR()
                else: # default 'rf'
                    model = RandomForestRegressor(n_estimators=100, random_state=42)
                
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                result_data = {
                    "type": "regression",
                    "model": model_name,
                    "metrics": {
                        "R2 Score (ì„¤ëª…ë ¥)": f"{r2:.4f}",
                        "MSE (ì˜¤ì°¨ì œê³±í‰ê· )": f"{mse:.4f}"
                    }
                }
            else:
                # --- [CASE 2] ë¶„ë¥˜ (Classification) ---
                # ğŸ’¡ í•µì‹¬: í”„ë¡ íŠ¸ì—ì„œ 'linear'ë¼ê³  ë³´ë‚´ë„, ë¶„ë¥˜ ë¬¸ì œë¼ë©´ -> LogisticRegression ì‹¤í–‰
                if model_name == 'linear' or model_name == 'logistic':
                    model = LogisticRegression(max_iter=1000)
                elif model_name == 'gb':
                    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
                elif model_name == 'svm':
                    model = SVC()
                else: # default 'rf'
                    model = RandomForestClassifier(n_estimators=100, random_state=42)

                # í•™ìŠµ ë° í‰ê°€
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                result_data = {
                    "type": "classification",
                    "model": model_name,
                    "metrics": {
                        "Accuracy (ì •í™•ë„)": f"{accuracy * 100:.2f}%"
                    }
                }

            # 7. ğŸ’¡ ì¤‘ìš” ë³€ìˆ˜ ì¶”ì¶œ (ëª¨ë¸ë³„ ì†ì„± ì°¨ì´ ì²˜ë¦¬)
            importances = {}
            
            # (1) íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ (feature_importances_)
            if hasattr(model, 'feature_importances_'):
                importances = dict(zip(X.columns, model.feature_importances_))
            
            # (2) ì„ í˜• ëª¨ë¸ (coef_) - ì ˆëŒ€ê°’ í¬ê¸°ë¡œ ì¤‘ìš”ë„ ê°€ëŠ 
            elif hasattr(model, 'coef_'):
                # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¼ ê²½ìš° ì²« ë²ˆì§¸ í´ë˜ìŠ¤ ê¸°ì¤€ í˜¹ì€ í‰ê·  ì‚¬ìš© ë“± ë³µì¡í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ë‹¨ìˆœí™”
                coefs = model.coef_
                if coefs.ndim > 1: 
                    coefs = coefs[0] # ì²« ë²ˆì§¸ í´ë˜ìŠ¤ ë˜ëŠ” ì°¨ì›
                importances = dict(zip(X.columns, np.abs(coefs)))
            
            # (3) SVM ë“± ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° -> ë¹ˆ ë”•ì…”ë„ˆë¦¬

            if importances:
                sorted_importances = dict(sorted(importances.items(), key=lambda item: item[1], reverse=True))
                result_data["feature_importances"] = sorted_importances
            else:
                result_data["feature_importances"] = {}


            # ğŸ’¡ [ì‹ ê·œ ê¸°ëŠ¥] ê²°ê³¼ í•´ì„ ë° ì„¤ëª… ìƒì„± ë¡œì§
            explanation = []
            
            # 1. ì„±ëŠ¥ í‰ê°€ í•´ì„
            if is_regression:
                r2_val = r2 
                if r2_val >= 0.8:
                    grade = "ì•„ì£¼ í›Œë¥­í•´ìš”! ğŸŒŸ"
                    desc = f"AIê°€ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì•„ì£¼ ì˜ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. (ì„¤ëª…ë ¥: {r2_val*100:.1f}%)<br>ì´ ëª¨ë¸ì€ ì‹¤ì „ì—ì„œ ì‚¬ìš©í•´ë„ ì¢‹ì„ ë§Œí¼ ë¯¿ìŒì§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
                elif r2_val >= 0.5:
                    grade = "ì¤€ìˆ˜í•©ë‹ˆë‹¤. âœ…"
                    desc = f"ë°ì´í„°ì˜ íë¦„ì„ ì ˆë°˜ ì´ìƒ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. (ì„¤ëª…ë ¥: {r2_val*100:.1f}%)<br>ë” ë§ì€ ë°ì´í„°ë¥¼ ëª¨ìœ¼ë©´ ì„±ëŠ¥ì´ í›¨ì”¬ ì¢‹ì•„ì§ˆ ê±°ì˜ˆìš”."
                else:
                    grade = "ë…¸ë ¥ì´ í•„ìš”í•´ìš”. ğŸ˜…"
                    desc = f"ì•„ì§ ì˜ˆì¸¡ë ¥ì´ ë‹¤ì†Œ ë‚®ìŠµë‹ˆë‹¤. (ì„¤ëª…ë ¥: {r2_val*100:.1f}%)<br>ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ í•˜ê±°ë‚˜, ì´ìƒì¹˜ë¥¼ ì œê±°í•´ ë³´ì„¸ìš”."
                
                explanation.append(f"<strong>[{grade}]</strong> {desc}")
            
            else: # ë¶„ë¥˜ (Classification)
                acc_val = accuracy * 100
                if acc_val >= 90:
                    grade = "ì²œì¬ì ì¸ ìˆ˜ì¤€ì´ì—ìš”! ğŸš€"
                    desc = f"ì •ë‹µë¥ ì´ {acc_val:.1f}%ì…ë‹ˆë‹¤.<br>ê±°ì˜ ëª¨ë“  ì¼€ì´ìŠ¤ë¥¼ ì •í™•í•˜ê²Œ ë§ì¶”ê³  ìˆë„¤ìš”!"
                elif acc_val >= 70:
                    grade = "ì“¸ë§Œí•˜ë„¤ìš”! ğŸ‘"
                    desc = f"ì •ë‹µë¥ ì´ {acc_val:.1f}%ì…ë‹ˆë‹¤.<br>ê¸°ë³¸ì ì¸ ë¶„ë¥˜ëŠ” ì˜ í•´ë‚´ê³  ìˆìŠµë‹ˆë‹¤."
                else:
                    grade = "ì¡°ê¸ˆ ì•„ì‰¬ì›Œìš”. ğŸ¤”"
                    desc = f"ì •ë‹µë¥ ì´ {acc_val:.1f}%ì…ë‹ˆë‹¤.<br>ë™ì „ ë˜ì§€ê¸°ë³´ë‹¤ëŠ” ë‚«ì§€ë§Œ, ê°œì„ ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤."
                
                explanation.append(f"<strong>[{grade}]</strong> {desc}")

            # 2. ì¤‘ìš” ë³€ìˆ˜ í•´ì„
            if result_data.get("feature_importances"):
                top_3 = list(result_data["feature_importances"].keys())[:3]
                # ë³€ìˆ˜ ì´ë¦„ë“¤ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ []ë¡œ ê°ì‹¸ê¸°
                top_3_str = ", ".join([f"<b>[{f}]</b>" for f in top_3])
                
                insight = f"<br><br>ğŸ’¡ <b>ë¶„ì„ íŒ</b>: ê²°ê³¼('{target_col}')ë¥¼ ê²°ì •ì§“ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ìš”ì¸ì€ {top_3_str} ìˆœì„œì…ë‹ˆë‹¤."
                insight += f"<br>íŠ¹íˆ <b>'{top_3[0]}'</b> ë°ì´í„°ê°€ ë³€í•˜ë©´ ê²°ê³¼ë„ í¬ê²Œ ë‹¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë‹ˆ ì£¼ëª©í•˜ì„¸ìš”!"
                explanation.append(insight)
            else:
                explanation.append("<br><br>âš ï¸ ì´ ëª¨ë¸ì€ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ì§€ ì•Šì•„, ì–´ë–¤ ìš”ì¸ì´ ì¤‘ìš”í•œì§€ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")

            # ê²°ê³¼ ë°ì´í„°ì— ì„¤ëª… ì¶”ê°€
            result_data["explanation"] = "".join(explanation)

            # ğŸ’¡ [ì‹ ê·œ ê¸°ëŠ¥] ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ë¹„êµ ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ìµœëŒ€ 10ê°œ)
            sample_size = 10
            # ì¸ë±ìŠ¤ ë¦¬ì…‹ì„ ìœ„í•´ DataFrame/Seriesë¡œ ë³€í™˜ ë³´ì¥
            y_test_reset = pd.Series(y_test).reset_index(drop=True)
            y_pred_reset = pd.Series(y_pred).reset_index(drop=True)
            
            samples = []
            
            # (1) ë¶„ë¥˜ ë¬¸ì œì¼ ê²½ìš°: ë¼ë²¨ ë³µì› (0, 1 -> 'Yes', 'No')
            if not is_regression and 'le_y' in locals() and le_y is not None:
                # LabelEncoderê°€ ìˆë‹¤ë©´ ì›ë˜ ë¬¸ìì—´ë¡œ ë³µêµ¬
                actual_values = le_y.inverse_transform(y_test_reset[:sample_size].astype(int))
                pred_values = le_y.inverse_transform(y_pred_reset[:sample_size].astype(int))
            else:
                # íšŒê·€ê±°ë‚˜ ì¸ì½”ë”© ì•ˆ ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                actual_values = y_test_reset[:sample_size].values
                pred_values = y_pred_reset[:sample_size].values

            # (2) ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            for i in range(min(len(actual_values), sample_size)):
                actual = actual_values[i]
                pred = pred_values[i]
                
                # íšŒê·€ì˜ ê²½ìš° ì†Œìˆ˜ì  ì •ë¦¬
                if is_regression:
                    actual = round(float(actual), 2)
                    pred = round(float(pred), 2)
                    diff = round(abs(actual - pred), 2) # ì˜¤ì°¨
                    is_correct = diff  # íšŒê·€ì—ì„œëŠ” ì˜¤ì°¨ê°’ ìì²´
                else:
                    # ë¶„ë¥˜ëŠ” ë§ìŒ/í‹€ë¦¼ ì—¬ë¶€ (True/False)
                    is_correct = (str(actual) == str(pred))
                
                samples.append({
                    "id": i + 1,
                    "actual": actual,
                    "predicted": pred,
                    "is_correct": is_correct # ë¶„ë¥˜: bool, íšŒê·€: ì˜¤ì°¨ê°’(float)
                })

            result_data["samples"] = samples

            return Response(result_data)

        except Exception as e:
            import traceback
            traceback.print_exc()
            return Response({"error": f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}, status=500)